# tasks/story_generation/data_handler.py

from datasets import load_dataset


def load_and_prepare_dataset(dataset_name, tokenizer, train_sample_size, eval_sample_size, max_length,
                             dataset_config_name=None, **kwargs):
    """
    ä¸º "Prompt-to-Story" Causal LM ä»»åŠ¡åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†ã€‚

    Args:
        dataset_name (str): æ•°æ®é›†åç§°ã€‚
        tokenizer: Hugging Face Tokenizer å®ä¾‹ã€‚
        train_sample_size (int): è®­ç»ƒé›†é‡‡æ ·å¤§å°ã€‚
        eval_sample_size (int): è¯„ä¼°é›†é‡‡æ ·å¤§å°ã€‚
        max_length (int): åˆ†è¯æ—¶çš„æœ€å¤§é•¿åº¦ã€‚
        dataset_config_name (str, optional): æ•°æ®é›†çš„ç‰¹å®šé…ç½®ã€‚
        **kwargs: æ¥æ”¶ main.py ä¼ å…¥çš„å…¶ä»–å¤šä½™å‚æ•°ï¼Œä»¥ä¿æŒæ¥å£å…¼å®¹æ€§ã€‚

    Returns:
        tuple: (tokenized_train_dataset, tokenized_eval_dataset, num_labels)
               å¯¹äºCausalLMä»»åŠ¡, num_labels ä¸º Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›† '{dataset_name}'...")
    full_dataset = load_dataset(dataset_name, name=dataset_config_name)
    train_dataset = full_dataset['train']
    eval_dataset = full_dataset['validation']

    if train_sample_size:
        print(f"ğŸ” é‡‡æ · {train_sample_size} æ¡æ•°æ®ä½œä¸ºè®­ç»ƒé›†...")
        train_dataset = train_dataset.select(range(train_sample_size))

    if eval_sample_size:
        print(f"ğŸ” é‡‡æ · {eval_sample_size} æ¡æ•°æ®ä½œä¸ºè¯„ä¼°é›†...")
        eval_dataset = eval_dataset.select(range(eval_sample_size))

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, è¯„ä¼°é›†å¤§å°: {len(eval_dataset)}")

    def tokenize_function(examples):
        """
        ### æ ¸å¿ƒæ”¹åŠ¨ ###
        ç›®æ ‡: å°† 'prompt' å’Œ 'story' æ‹¼æ¥ä¸ºä¸€æ¡å®Œæ•´çš„åºåˆ—ã€‚
        æ–¹æ³•: 1. æ‹¼æ¥æ–‡æœ¬: prompt + eos_token + story + eos_token
              2. ç›´æ¥å¯¹æ‹¼æ¥åçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚
        æˆ‘ä»¬ä¸å†æ‰‹åŠ¨åˆ›å»º labelsï¼Œè¿™é¡¹å·¥ä½œå°†å®Œå…¨äº¤ç»™ DataCollatorForLanguageModelingã€‚
        """
        # 1. å°† prompt å’Œ story æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„è¾“å…¥æ–‡æœ¬
        #    eos_token ç”¨äºåˆ†éš” prompt å’Œ storyï¼Œå¹¶æ ‡è®°åºåˆ—ç»“æŸ
        full_texts = [
            p + tokenizer.eos_token + s + tokenizer.eos_token
            for p, s in zip(examples['prompt'], examples['story'])
        ]

        # 2. å¯¹æ‹¼æ¥åçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯
        model_inputs = tokenizer(
            full_texts,
            truncation=True,
            max_length=max_length,
            padding=False,  # åŠ¨æ€å¡«å……ç”± DataCollator åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­å¤„ç†ï¼Œæ•ˆç‡æ›´é«˜
        )
        return model_inputs

    print("ğŸ§  ----------------------------------------------------")
    print("ğŸ§  æ­£åœ¨ä»¥ 'Prompt-to-Story' æ¨¡å¼å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯...")
    print("ğŸ§  (è¾“å…¥ = prompt + story, æ ‡ç­¾ç”± DataCollator è‡ªåŠ¨ç”Ÿæˆ)") # æ›´æ–°äº†æç¤ºä¿¡æ¯
    print("ğŸ§  ----------------------------------------------------")

    tokenized_train_dataset = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="å¤„ç†è®­ç»ƒé›†..."
    )
    tokenized_eval_dataset = eval_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=eval_dataset.column_names,
        desc="å¤„ç†è¯„ä¼°é›†..."
    )

    print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    return tokenized_train_dataset, tokenized_eval_dataset, None