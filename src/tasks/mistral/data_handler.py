from datasets import load_dataset
import os


def load_and_prepare_dataset(dataset_name, tokenizer, train_sample_size=None, eval_sample_size=None, 
                           max_length=2048, **kwargs):
    """
    ä¸º Mistral æ¨¡å‹åŠ è½½å¹¶å‡†å¤‡ UltraChat 200k æ•°æ®é›†ç”¨äºé¢„è®­ç»ƒã€‚
    
    Args:
        dataset_name (str): æ•°æ®é›†åç§° (HuggingFaceH4/ultrachat_200k)
        tokenizer: Hugging Face Tokenizer å®ä¾‹
        train_sample_size (int): è®­ç»ƒé›†é‡‡æ ·å¤§å°
        eval_sample_size (int): è¯„ä¼°é›†é‡‡æ ·å¤§å°
        max_length (int): åˆ†è¯æ—¶çš„æœ€å¤§é•¿åº¦
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        tuple: (tokenized_train_dataset, tokenized_eval_dataset, None)
    """
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½ UltraChat 200k æ•°æ®é›†...")
    
    # åŠ è½½ UltraChat 200k æ•°æ®é›†
    # è¯¥æ•°æ®é›†åŒ…å« train_sft, test_sft, train_gen, test_gen å››ä¸ªå­é›†
    dataset = load_dataset(dataset_name)
    
    # ä½¿ç”¨ SFT (Supervised Fine-Tuning) æ•°æ®é›†
    # train_sft: 207,865 ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
    # test_sft: 23,110 ä¸ªæ ·æœ¬ç”¨äºè¯„ä¼°
    train_dataset = dataset['train_sft']
    eval_dataset = dataset['test_sft']

    if train_sample_size:
        print(f"ğŸ” é‡‡æ · {train_sample_size} æ¡æ•°æ®ä½œä¸ºè®­ç»ƒé›†...")
        train_dataset = train_dataset.select(range(train_sample_size))
    if eval_sample_size:
        print(f"ğŸ” é‡‡æ · {eval_sample_size} æ¡æ•°æ®ä½œä¸ºè¯„ä¼°é›†...")
        eval_dataset = eval_dataset.select(range(eval_sample_size))

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, è¯„ä¼°é›†å¤§å°: {len(eval_dataset)}")

    if tokenizer.chat_template is None:
        print("ğŸ”§ Tokenizer ç¼ºå°‘èŠå¤©æ¨¡æ¿ï¼Œæ­£åœ¨æ‰‹åŠ¨è®¾ç½® Mistral/Llama-2 æ ¼å¼çš„æ¨¡æ¿...")

        # è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ Jinja2 æ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–å¯¹è¯
        MISTRAL_CHAT_TEMPLATE = (
            "{% for message in messages %}"
            "{% if message['role'] == 'user' %}"
            "{{ '<s>[INST] ' + message['content'] + ' [/INST]' }}"
            "{% elif message['role'] == 'assistant' %}"
            "{{ message['content'] + '</s>' }}"
            "{% endif %}"
            "{% endfor %}"
        )
        tokenizer.chat_template = MISTRAL_CHAT_TEMPLATE
        print("âœ… èŠå¤©æ¨¡æ¿è®¾ç½®æˆåŠŸï¼")

    def tokenize_function(examples):
        """
        å°†å¯¹è¯æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼å¹¶åˆ†è¯ã€‚
        
        ç›®æ ‡: å°†å¯¹è¯æ¶ˆæ¯è½¬æ¢ä¸ºå®Œæ•´çš„è¾“å…¥åºåˆ—ç”¨äºé¢„è®­ç»ƒ
        æ–¹æ³•: 1. ä½¿ç”¨ apply_chat_template æ ¼å¼åŒ–å¯¹è¯
              2. å¯¹æ ¼å¼åŒ–åçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯
              3. è¿”å› input_ids ç”¨äºé¢„è®­ç»ƒ
        """
        # 1. ä½¿ç”¨å¯¹è¯æ¨¡æ¿æ ¼å¼åŒ–æ¶ˆæ¯
        formatted_texts = []
        for messages in examples['messages']:
            # apply_chat_template å°†æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
            # ä¾‹å¦‚: "<s>[INST] user message [/INST] assistant response </s>"
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            formatted_texts.append(formatted_text)

        # 2. å¯¹æ ¼å¼åŒ–åçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯
        model_inputs = tokenizer(
            formatted_texts,
            truncation=True,
            max_length=max_length,
            padding=False,  # åŠ¨æ€å¡«å……ç”± DataCollator å¤„ç†
        )
        
        return model_inputs

    print("ğŸ§  ----------------------------------------------------")
    print("ğŸ§  æ­£åœ¨ä»¥å¯¹è¯æ ¼å¼å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯...")
    print("ğŸ§  (è¾“å…¥ = æ ¼å¼åŒ–çš„å¯¹è¯, æ ‡ç­¾ç”± DataCollator è‡ªåŠ¨ç”Ÿæˆ)")
    print("ğŸ§  ----------------------------------------------------")

    # è®¾ç½®å¤šçº¿ç¨‹å¤„ç†ï¼Œæœ€å¤šä½¿ç”¨ 8 ä¸ª CPU æ ¸å¿ƒ
    num_proc = min(8, os.cpu_count())
    print(f"ğŸ”§ ä½¿ç”¨ {num_proc} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†...")

    tokenized_train_dataset = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=train_dataset.column_names,
        num_proc=num_proc,
        desc="å¤„ç†è®­ç»ƒé›†..."
    )
    
    tokenized_eval_dataset = eval_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=eval_dataset.column_names,
        num_proc=num_proc,
        desc="å¤„ç†è¯„ä¼°é›†..."
    )

    print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    return tokenized_train_dataset, tokenized_eval_dataset, None