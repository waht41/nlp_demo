import os
import yaml
import shutil
import subprocess
import argparse
import pandas as pd
from datetime import datetime

from transformers import TrainingArguments, Trainer, AutoTokenizer

from model_handler import load_model_and_tokenizer
from data_handler import load_and_prepare_dataset
from metrics import compute_metrics


def get_git_info(output_dir):
    """è·å–å½“å‰çš„ git commit hash å’Œæœªæäº¤çš„ä¿®æ”¹ï¼Œå¹¶ä¿å­˜åˆ°ç»“æœç›®å½•"""
    try:
        # 1. è·å– git commit hash
        git_hash = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip().decode('utf-8')

        # 2. å°†æœªæäº¤çš„ä¿®æ”¹ä¿å­˜ä¸ºä¸€ä¸ª patch æ–‡ä»¶
        # è¿™å¯¹äºå¤ç°è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒè®°å½•äº†åœ¨æœ€åä¸€æ¬¡ commit ä¹‹åçš„æ‰€æœ‰ä»£ç æ”¹åŠ¨
        diff_path = os.path.join(output_dir, "code_changes.patch")
        with open(diff_path, "w") as f:
            subprocess.run(['git', 'diff', 'HEAD'], stdout=f)

        print(f"âœ… Git Hash ({git_hash}) å·²è®°å½•ã€‚")
        if os.path.getsize(diff_path) > 0:
            print(f"âš ï¸ å‘ç°æœªæäº¤çš„ä»£ç ä¿®æ”¹ï¼Œå·²ä¿å­˜è‡³: {diff_path}")
        else:
            os.remove(diff_path)  # å¦‚æœæ²¡æœ‰æ”¹åŠ¨ï¼Œåˆ™åˆ é™¤ç©ºçš„ patch æ–‡ä»¶

        return git_hash
    except subprocess.CalledProcessError:
        print("â“ æœªèƒ½è·å– Git ä¿¡æ¯ã€‚å¯èƒ½ä¸æ˜¯ä¸€ä¸ª Git ä»“åº“ã€‚")
        return "N/A"


def main(config_path: str):
    """ä¸»å‡½æ•°ï¼Œä»é…ç½®æ–‡ä»¶æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹"""

    # --- 1. åŠ è½½é…ç½® ---
    print(f"ğŸ“– ä» '{config_path}' åŠ è½½é…ç½®...")
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    # ä»é…ç½®ä¸­æå–å‚æ•°
    metadata_cfg = config.get('metadata', {})
    model_data_cfg = config.get('model_data', {})
    training_cfg = config.get('training', {})

    # --- 2. åˆ›å»ºå”¯ä¸€çš„å®éªŒç›®å½•å’ŒID ---
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    run_id = f"{timestamp}_{metadata_cfg.get('run_name', 'unnamed_run')}"

    # ç»“æœå’Œæ—¥å¿—å°†ä¿å­˜åœ¨ä»¥ run_id å‘½åçš„ä¸“å±æ–‡ä»¶å¤¹ä¸­
    OUTPUT_DIR = os.path.join("./results", run_id)
    LOGGING_DIR = os.path.join("./logs", run_id)

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(LOGGING_DIR, exist_ok=True)
    print(f"ğŸš€ åˆ›å»ºå®éªŒè¿è¡Œ ID: {run_id}")
    print(f"   - æ¨¡å‹å’Œè¯„ä¼°ç»“æœå°†ä¿å­˜è‡³: {OUTPUT_DIR}")
    print(f"   - é…ç½®æ–‡ä»¶ã€ä»£ç å·®å¼‚å’ŒTensorBoardæ—¥å¿—å°†ä¿å­˜è‡³: {LOGGING_DIR}")

    # --- 3. è®°å½•ä»£ç å’Œé…ç½®çŠ¶æ€ (ä¸ºäº†100%å¯å¤ç°) ---
    # 3.1 å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°æ—¥å¿—ç›®å½•
    shutil.copy(config_path, os.path.join(LOGGING_DIR, "config.yaml"))

    # 3.2 è·å– Git çŠ¶æ€å¹¶ä¿å­˜ä»£ç å·®å¼‚
    git_hash = get_git_info(LOGGING_DIR)

    # --- 4. åŠ è½½æ•°æ®é›† ---
    print("\n" + "=" * 20 + " æ­£åœ¨åŠ è½½æ•°æ®é›† " + "=" * 20)
    tokenizer = AutoTokenizer.from_pretrained(model_data_cfg['model_checkpoint'])

    train_dataset, eval_dataset, num_labels = load_and_prepare_dataset(
        dataset_name=model_data_cfg['dataset_name'],
        tokenizer=tokenizer,
        train_sample_size=model_data_cfg.get('train_sample_size'),
        eval_sample_size=model_data_cfg.get('eval_sample_size')
    )
    print(f"ä»æ•°æ®é›†ä¸­æ¨æ–­å‡ºçš„ num_labels: {num_labels}")

    # --- 5. åŠ è½½æ¨¡å‹ ---
    print("\n" + "=" * 20 + " æ­£åœ¨åŠ è½½æ¨¡å‹ " + "=" * 20)
    model, _ = load_model_and_tokenizer(
        model_checkpoint=model_data_cfg['model_checkpoint'],
        num_labels=num_labels
    )

    # --- 6. é…ç½®è®­ç»ƒå‚æ•° ---
    print("\n" + "=" * 20 + " æ­£åœ¨é…ç½®è®­ç»ƒå‚æ•° " + "=" * 20)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        logging_dir=LOGGING_DIR,
        report_to="tensorboard",
        run_name=run_id,

        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–è®­ç»ƒå‚æ•°
        num_train_epochs=training_cfg['num_train_epochs'],
        per_device_train_batch_size=training_cfg['per_device_train_batch_size'],
        per_device_eval_batch_size=training_cfg.get('per_device_eval_batch_size',
                                                    training_cfg['per_device_train_batch_size']),
        learning_rate=float(training_cfg['learning_rate']),
        weight_decay=training_cfg.get('weight_decay', 0.0),
        max_grad_norm=training_cfg.get('max_grad_norm', 1.0),
        warmup_ratio=training_cfg.get('warmup_ratio', 0.0),

        # è¯„ä¼°ã€ä¿å­˜å’Œæ—¥å¿—ç­–ç•¥
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        logging_strategy="steps",
        logging_steps=50,
    )

    # --- 7. åˆå§‹åŒ–å¹¶å¯åŠ¨è®­ç»ƒå™¨ ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    print("\n" + "=" * 40)
    print("          ğŸ”¥ å¼€å§‹æ¨¡å‹è®­ç»ƒ ğŸ”¥          ")
    print("=" * 40 + "\n")

    trainer.train()

    print("\n" + "=" * 40)
    print("          âœ… è®­ç»ƒå®Œæˆ âœ…          ")
    print("=" * 40 + "\n")

    # --- 8. åœ¨è¯„ä¼°é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° ---
    print("åœ¨æœ€ç»ˆè¯„ä¼°é›†ä¸Šè¿›è¡Œè¯„ä¼°...")
    final_metrics = trainer.evaluate(eval_dataset)
    print("æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(final_metrics)

    # --- 9. å°†å®éªŒæ‘˜è¦è®°å½•åˆ°ä¸­å¤®CSVæ–‡ä»¶ ---
    summary = {
        'run_id': run_id,
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'run_name': metadata_cfg.get('run_name', ''),
        'description': metadata_cfg.get('description', ''),
        'git_hash': git_hash,
        'model_checkpoint': model_data_cfg['model_checkpoint'],
        'dataset_name': model_data_cfg['dataset_name'],
        'learning_rate': training_cfg['learning_rate'],
        'epochs': training_cfg['num_train_epochs'],
        'batch_size': training_cfg['per_device_train_batch_size'],
        'final_eval_accuracy': final_metrics.get('eval_accuracy'),
        'final_eval_loss': final_metrics.get('eval_loss'),
        'results_path': OUTPUT_DIR,
    }

    log_file = "./experiments.csv"
    summary_df = pd.DataFrame([summary])

    # çº¿ç¨‹å®‰å…¨åœ°è¿½åŠ åˆ° CSV æ–‡ä»¶
    if not os.path.exists(log_file):
        summary_df.to_csv(log_file, index=False)
    else:
        summary_df.to_csv(log_file, mode='a', header=False, index=False)

    print("\n" + "=" * 40)
    print(f"   ğŸ“Š å®éªŒæ€»ç»“å·²è®°å½•åˆ°ä¸­å¤®æ—¥å¿—: {log_file}   ")
    print("=" * 40 + "\n")

    # --- 10. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = os.path.join(OUTPUT_DIR, "final_model")
    trainer.save_model(final_model_path)
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
    print(f"è¦æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir ./logs")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä» YAML é…ç½®æ–‡ä»¶è¿è¡Œæ¨¡å‹è®­ç»ƒã€‚")
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="æŒ‡å‘å®éªŒé…ç½®æ–‡ä»¶çš„è·¯å¾„ (ä¾‹å¦‚: configs/baseline_experiment.yaml)"
    )
    args = parser.parse_args()
    main(args.config)
