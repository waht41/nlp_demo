import os
import yaml
import shutil
import argparse
import pandas as pd
import torch
from datetime import datetime
import importlib
from functools import partial

from transformers import (
    TrainingArguments, Trainer,
    Seq2SeqTrainingArguments, Seq2SeqTrainer,
    AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, AutoModelForCausalLM,
    DataCollatorForSeq2Seq, DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

from trainer_callback import DistributionLoggingCallback, PerplexityLoggingCallback
from utils.git import get_git_info


# --- ä¸»å‡½æ•°å¼€å§‹ ---
def main(task_name: str, resume_from: str = None):
    """ä¸»å‡½æ•°ï¼Œä»æŒ‡å®šçš„ä»»åŠ¡ç›®å½•æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹

    Args:
        task_name: è¦æ‰§è¡Œçš„ä»»åŠ¡åç§°
        resume_from: å¯é€‰ï¼ŒæŒ‡å®šè¦ä»å“ªä¸ªæ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
    """

    # --- 1. åŠ¨æ€åŠ è½½ä»»åŠ¡æ¨¡å—å’Œé…ç½® ---
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_name}")
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    TASKS_DIR = os.path.join(SCRIPT_DIR, "tasks")
    config_path = os.path.join(TASKS_DIR, task_name, "config.yaml")

    print(f"ğŸ“– ä» '{config_path}' åŠ è½½é…ç½®...")
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    metadata_cfg = config.get('metadata', {})
    model_data_cfg = config.get('model_data', {})
    training_cfg = config.get('training', {})

    task_type = model_data_cfg.get('task_type', 'classification')
    if task_type not in ['classification', 'seq2seq', 'causalLM']:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")

    print(f"æ£€æµ‹åˆ°ä»»åŠ¡ç±»å‹: {task_type}")

    try:
        # åŠ¨æ€å¯¼å…¥ç‰¹å®šä»»åŠ¡çš„æ•°æ®å¤„ç†æ¨¡å—ï¼ˆå¿…éœ€ï¼‰
        data_handler_module = importlib.import_module(f"tasks.{task_name}.data_handler")
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦éœ€è¦å¯¼å…¥metricsæ¨¡å—
        ignore_metrics = training_cfg.get('ignore_compute_metric', False)
        metrics_module = None if ignore_metrics else importlib.import_module(f"tasks.{task_name}.metrics")
        if ignore_metrics:
            print("ğŸ“Š è·³è¿‡metricsæ¨¡å—å¯¼å…¥ï¼ˆæ ¹æ®é…ç½®ignore_compute_metric=trueï¼‰")
    except ModuleNotFoundError as e:
        print(f"é”™è¯¯: å¯¼å…¥ä»»åŠ¡ '{task_name}' ç›¸å…³æ¨¡å—æ—¶å¤±è´¥ã€‚")
        print(f"å…·ä½“é”™è¯¯: {str(e)}")
        print("å¯èƒ½çš„åŸå› :")
        print("1. tasks/ ç›®å½•ä¸‹æ²¡æœ‰å¯¹åº”çš„ä»»åŠ¡æ–‡ä»¶å¤¹")
        print("2. ä»»åŠ¡æ–‡ä»¶å¤¹ä¸­ç¼ºå°‘å¿…éœ€çš„ data_handler.py æˆ– metrics.py æ–‡ä»¶")
        print("3. ä»»åŠ¡æ¨¡å—ä¸­å¼•ç”¨çš„ä¾èµ–åŒ…æœªå®‰è£…ï¼Œè¯·æ£€æŸ¥ requirements.txt å¹¶å®‰è£…æ‰€éœ€ä¾èµ–")
        return

    # --- 2. åˆ›å»ºå”¯ä¸€çš„å®éªŒç›®å½•å’ŒID ---
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    # è®©run_idåŒ…å«ä»»åŠ¡åï¼Œæ›´æ¸…æ™°
    run_id = f"{timestamp}_{task_name}_{metadata_cfg.get('run_name', 'unnamed_run')}"

    OUTPUT_DIR = os.path.join("./results", run_id)
    LOGGING_DIR = os.path.join("./logs", run_id)

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(LOGGING_DIR, exist_ok=True)
    print(f"ğŸš€ åˆ›å»ºå®éªŒè¿è¡Œ ID: {run_id}")
    print(f"   - ç»“æœå°†ä¿å­˜è‡³: {OUTPUT_DIR}")
    print(f"   - æ—¥å¿—å°†ä¿å­˜è‡³: {LOGGING_DIR}")

    # --- 3. è®°å½•ä»£ç å’Œé…ç½®çŠ¶æ€ ---
    shutil.copy(config_path, os.path.join(LOGGING_DIR, "config.yaml"))
    git_hash = get_git_info(LOGGING_DIR)

    # --- 4. åŠ è½½æ•°æ®é›† ---
    print("\n" + "=" * 20 + " æ­£åœ¨åŠ è½½æ•°æ®é›† " + "=" * 20)
    tokenizer = AutoTokenizer.from_pretrained(model_data_cfg['model_checkpoint'], trust_remote_code=True)

    # è¿™é‡Œ data_handler.py å†…éƒ¨ä¼šå¤„ç†ä¸åŒä»»åŠ¡çš„é€»è¾‘
    # å°è£…é€šç”¨å‚æ•°ï¼Œé¿å…é‡å¤
    common_dataset_args = {
        'dataset_name': model_data_cfg['dataset_name'],
        'tokenizer': tokenizer,
        'train_sample_size': model_data_cfg.get('train_sample_size'),
        'eval_sample_size': model_data_cfg.get('eval_sample_size'),
        'dataset_config_name': model_data_cfg.get('dataset_config_name'),
    }

    if task_type == 'seq2seq':
        # åªæœ‰ seq2seq ä»»åŠ¡æ‰ä¼ é€’è¿™ä¸¤ä¸ªå‚æ•°
        datasets_and_labels = data_handler_module.load_and_prepare_dataset(
            **common_dataset_args,
            max_source_length=model_data_cfg.get('max_source_length'),
            max_target_length=model_data_cfg.get('max_target_length')
        )
    elif task_type == 'causalLM':
        # ä¸º causalLM ä»»åŠ¡è®¾ç½® pad_token
        # ç¡®ä¿ tokenizer æœ‰ eos_token
        if tokenizer.eos_token is None:
            tokenizer.eos_token = "<|endoftext|>"
            print('æ·»åŠ eos_token <|endoftext|>')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        datasets_and_labels = data_handler_module.load_and_prepare_dataset(
            **common_dataset_args,
            max_length=model_data_cfg.get('max_length', 1024)
        )
    else:
        # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨åŸæœ‰çš„å‚æ•°
        datasets_and_labels = data_handler_module.load_and_prepare_dataset(**common_dataset_args)
    # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œè¿”å› (train, eval, num_labels)
    # å¯¹äºS2Sä»»åŠ¡ï¼Œå¯ä»¥çº¦å®šè¿”å› (train, eval, None) å› ä¸º num_labels ä¸é€‚ç”¨
    train_dataset, eval_dataset, num_labels = datasets_and_labels
    if num_labels:
        print(f"ä»æ•°æ®é›†ä¸­æ¨æ–­å‡ºçš„ num_labels: {num_labels}")

    # --- 5. æ ¹æ® task_type åŠ è½½æ¨¡å‹ ---
    print("\n" + "=" * 20 + " æ­£åœ¨åŠ è½½æ¨¡å‹ " + "=" * 20)
    model_path = resume_from if resume_from else model_data_cfg['model_checkpoint']
    print(f"ğŸ“‚ ä» '{model_path}' åŠ è½½...")

    # åœ¨ causalLM é€»è¾‘å—å¤–éƒ¨å®šä¹‰ lora_configï¼Œä»¥ä¾¿ Trainer éƒ¨åˆ†å¯ä»¥è®¿é—®
    lora_config = None

    if task_type == 'seq2seq':
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    elif task_type == 'causalLM':
        # --- LLM é«˜æ•ˆå¾®è°ƒçš„æ ¸å¿ƒé€»è¾‘ ---
        quantization_cfg = training_cfg.get('quantization')
        bnb_config = None
        if quantization_cfg and quantization_cfg.get('load_in_4bit', False):
            print("ğŸ’¡ å¯ç”¨ 4-bit é‡åŒ–åŠ è½½...")
            compute_dtype = getattr(torch, quantization_cfg.get('bnb_4bit_compute_dtype', 'float16'))
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=quantization_cfg.get('bnb_4bit_use_double_quant', True),
                bnb_4bit_quant_type=quantization_cfg.get('bnb_4bit_quant_type', "nf4"),
                bnb_4bit_compute_dtype=compute_dtype
            )

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )

        if training_cfg.get('use_peft', False):
            print("ğŸš€ åº”ç”¨ PEFT (LoRA) é…ç½®...")
            model = prepare_model_for_kbit_training(model)
            peft_lora_cfg = training_cfg.get('peft_lora')
            if not peft_lora_cfg:
                raise ValueError("é…ç½®é”™è¯¯: use_peft=true ä½† peft_lora é…ç½®å—ä¸å­˜åœ¨ï¼")
            lora_config = LoraConfig(**peft_lora_cfg)
            model = get_peft_model(model, lora_config)
            print("LoRA æ¨¡å‹å‚æ•°:")
            model.print_trainable_parameters()
    else:  # é»˜è®¤ä¸º classification
        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)

    if model_data_cfg.get('force_contiguous', False):
        model = model.to_contiguous()

    # --- 6. é…ç½®è®­ç»ƒå‚æ•° ---
    print("\n" + "=" * 20 + " æ­£åœ¨é…ç½®è®­ç»ƒå‚æ•° " + "=" * 20)
    common_args = {
        'output_dir': OUTPUT_DIR, 'logging_dir': LOGGING_DIR, 'report_to': "tensorboard",
        'run_name': run_id, 'optim': training_cfg.get('optimizer', 'adamw_torch'),
        'gradient_accumulation_steps': training_cfg.get('gradient_accumulation_steps', 1),
        'num_train_epochs': training_cfg['num_train_epochs'],
        'per_device_train_batch_size': training_cfg['per_device_train_batch_size'],
        'per_device_eval_batch_size': training_cfg.get('per_device_eval_batch_size',
                                                       training_cfg['per_device_train_batch_size']),
        'learning_rate': float(training_cfg['learning_rate']),
        'weight_decay': training_cfg.get('weight_decay', 0.0),
        'max_grad_norm': training_cfg.get('max_grad_norm', 1.0),
        'warmup_ratio': training_cfg.get('warmup_ratio', 0.0),
        'lr_scheduler_type': training_cfg.get('lr_scheduler_type', 'linear'),
        'eval_strategy': training_cfg.get('eval_strategy', 'epoch'),
        'save_strategy': training_cfg.get('save_strategy', 'epoch'),
        'eval_steps': training_cfg.get('eval_steps') if training_cfg.get('eval_strategy') == 'steps' else None,
        'save_steps': training_cfg.get('save_steps') if training_cfg.get('save_strategy') == 'steps' else None,
        'load_best_model_at_end': True,
        'logging_strategy': "steps",
        'logging_steps': training_cfg.get('logging_steps', 50),
        'fp16': training_cfg.get('fp16', False),
        'bf16': training_cfg.get('bf16', False),
        'torch_compile': training_cfg.get('torch_compile', False),
    }
    compute_metrics_fn = partial(metrics_module.compute_metrics,
                                 tokenizer=tokenizer) if metrics_module and task_type == 'seq2seq' else \
        metrics_module.compute_metrics if metrics_module else None

    if task_type == 'seq2seq':
        training_args = Seq2SeqTrainingArguments(**common_args, predict_with_generate=True,
                                                 generation_max_length=model_data_cfg.get('max_target_length', 128))
    else:
        training_args = TrainingArguments(**common_args)

    # --- 7. åˆå§‹åŒ– Trainer ---
    callbacks = []
    if training_cfg.get('log_distribution'):
        callbacks.append(DistributionLoggingCallback())
    if training_cfg.get('log_ppl'):
        callbacks.append(PerplexityLoggingCallback())

    if task_type == 'seq2seq':
        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, pad_to_multiple_of=8)
        trainer = Seq2SeqTrainer(model=model, args=training_args, train_dataset=train_dataset,
                                 eval_dataset=eval_dataset,
                                 tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics_fn,
                                 callbacks=callbacks)
    elif task_type == 'causalLM':
        if training_cfg.get('use_peft', False):
            print("ä½¿ç”¨ TRL çš„ SFTTrainer è¿›è¡Œ PEFT å¾®è°ƒ...")
            trainer = SFTTrainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                peft_config=lora_config,
                data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8),
                callbacks=callbacks,
            )
        else:
            data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)
            trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,
                              tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics_fn,
                              callbacks=callbacks)
    else:  # classification
        trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,
                          compute_metrics=compute_metrics_fn, callbacks=callbacks)

    # --- 8. å¼€å§‹è®­ç»ƒ ---
    print("\n" + "=" * 40 + "\n          ğŸ”¥ å¼€å§‹æ¨¡å‹è®­ç»ƒ ğŸ”¥          \n" + "=" * 40 + "\n")
    train_kwargs = {'resume_from_checkpoint': resume_from} if resume_from else {}
    trainer.train(**train_kwargs)
    print("\n" + "=" * 40 + "\n          âœ… è®­ç»ƒå®Œæˆ âœ…          \n" + "=" * 40 + "\n")

    # --- 9. æœ€ç»ˆè¯„ä¼°å’Œæ—¥å¿—è®°å½• ---
    print("åœ¨æœ€ç»ˆè¯„ä¼°é›†ä¸Šè¿›è¡Œè¯„ä¼°...")
    final_metrics = trainer.evaluate(eval_dataset)
    print("æœ€ç»ˆè¯„ä¼°ç»“æœ:", final_metrics)

    summary = {
        'task': task_name,
        'run_id': run_id,
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'run_name': metadata_cfg.get('run_name', ''),
        'description': metadata_cfg.get('description', ''),
        'git_hash': git_hash,
        'model_checkpoint': model_data_cfg['model_checkpoint'],
        'dataset_name': model_data_cfg['dataset_name'],
        'learning_rate': training_cfg.get('learning_rate'),
        'epochs': training_cfg['num_train_epochs'],
        'batch_size': training_cfg['per_device_train_batch_size'],
        'final_eval_accuracy': final_metrics.get('eval_accuracy'),
        'final_eval_loss': final_metrics.get('eval_loss'),
        'results_path': OUTPUT_DIR,
        'addition': f'train from {resume_from}' if resume_from else '',
    }
    log_file = "./experiments.csv"
    summary_df = pd.DataFrame([summary])
    summary_df.to_csv(log_file, mode='a', header=not os.path.exists(log_file), index=False, encoding='utf-8-sig')
    print("\n" + "=" * 40)
    print(f"   ğŸ“Š å®éªŒæ€»ç»“å·²è®°å½•åˆ°ä¸­å¤®æ—¥å¿—: {log_file}   ")
    print("=" * 40 + "\n")

    # --- 10. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = os.path.join(OUTPUT_DIR, "final_model")
    trainer.save_model(final_model_path)
    if training_cfg.get('use_peft', False):  # å¦‚æœæ˜¯ LoRA è®­ç»ƒï¼Œé¢å¤–ä¿å­˜ tokenizer
        tokenizer.save_pretrained(final_model_path)
    print(f"æœ€ä½³æ¨¡å‹ï¼ˆæˆ–é€‚é…å™¨ï¼‰å·²ä¿å­˜è‡³: {final_model_path}")
    print(f"è¦æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir ./logs")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä»æŒ‡å®šçš„ä»»åŠ¡ç›®å½•è¿è¡Œæ¨¡å‹è®­ç»ƒã€‚")
    parser.add_argument(
        "--task", type=str, required=True,
        help="è¦æ‰§è¡Œçš„ä»»åŠ¡åç§° (å¿…é¡»æ˜¯ tasks/ ç›®å½•ä¸‹çš„ä¸€ä¸ªå­æ–‡ä»¶å¤¹å)"
    )
    parser.add_argument(
        "--resume", type=str,
        help="æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä»è¯¥æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ"
    )
    args = parser.parse_args()
    main(args.task, args.resume)