import numpy as np
import os
from datasets import load_dataset


def load_and_prepare_dataset(
        dataset_name,
        tokenizer,
        dataset_config_name,
        train_sample_size,
        eval_sample_size,
        max_source_length,
        max_target_length
):
    """
    åŠ è½½å¹¶é¢„å¤„ç†é€‚ç”¨äºä¸­è‹±åŒå‘ç¿»è¯‘çš„æ•°æ®é›†ã€‚

    Args:
        dataset_name (str): Hugging Face Hubä¸Šçš„æ•°æ®é›†åç§°ã€‚
        tokenizer: Hugging Face Tokenizerå®ä¾‹ã€‚
        dataset_config_name (str): æ•°æ®é›†çš„ç‰¹å®šé…ç½®åç§°ï¼ˆä¾‹å¦‚è¯­è¨€å¯¹ï¼‰ã€‚
        train_sample_size (int): è®­ç»ƒé›†é‡‡æ ·å¤§å°ã€‚
        eval_sample_size (int): è¯„ä¼°é›†é‡‡æ ·å¤§å°ã€‚
        max_source_length (int): è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚
        max_target_length (int): è¾“å‡ºåºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚

    Returns:
        tuple: åŒ…å«å¤„ç†åçš„è®­ç»ƒé›†ã€è¯„ä¼°é›†å’ŒNoneï¼ˆå› ä¸ºS2Sä»»åŠ¡æ²¡æœ‰num_labelsï¼‰ã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨ä» Hub åŠ è½½æ•°æ®é›†: {dataset_name}, é…ç½®: {dataset_config_name}")
    # 1. åŠ è½½æ•°æ®é›†
    raw_datasets = load_dataset(dataset_name, name=dataset_config_name)

    # 2. å®šä¹‰ä»»åŠ¡å‰ç¼€ï¼Œè¿™æ˜¯è®©æ¨¡å‹çŸ¥é“ç¿»è¯‘æ–¹å‘çš„å…³é”®
    prefix_zh_to_en = "å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡: "
    prefix_en_to_zh = "translate English to Chinese: "

    def preprocess_function(examples):
        """å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¸ºåŒå‘ç¿»è¯‘åˆ›å»ºæ ·æœ¬"""
        inputs = []
        targets = []

        # examples['translation'] æ˜¯ä¸€ä¸ªlist of dict, æ¯ä¸ªdictæ˜¯ {'en': '...', 'zh': '...'}
        for ex in examples["translation"]:
            # è·³è¿‡å¯èƒ½å­˜åœ¨çš„ç©ºæ•°æ®
            if not ex['en'] or not ex['zh']:
                continue

            # åˆ›å»º ä¸­æ–‡ -> è‹±æ–‡ çš„æ ·æœ¬
            inputs.append(prefix_zh_to_en + ex["zh"].strip())
            targets.append(ex["en"].strip())

            # åˆ›å»º è‹±æ–‡ -> ä¸­æ–‡ çš„æ ·æœ¬
            inputs.append(prefix_en_to_zh + ex["en"].strip())
            targets.append(ex["zh"].strip())

        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç 
        model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True)

        # ä½¿ç”¨ target_tokenizer ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯¹ç›®æ ‡æ–‡æœ¬è¿›è¡Œç¼–ç 
        with tokenizer.as_target_tokenizer():
            labels = tokenizer(targets, max_length=max_target_length, truncation=True)

        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    # 3. å…ˆå¯¹åŸå§‹æ•°æ®è¿›è¡Œé‡‡æ ·
    train_dataset = raw_datasets["train"]
    eval_dataset = raw_datasets["validation"]  # é€šå¸¸ä½¿ç”¨validationé›†åšè¯„ä¼°

    if train_sample_size:
        print(f"ğŸ”ª å¯¹è®­ç»ƒé›†è¿›è¡Œé‡‡æ ·ï¼Œä¿ç•™ {train_sample_size} æ¡æ ·æœ¬...")
        # ä¸ºäº†ä¿è¯å¤šæ ·æ€§ï¼Œéšæœºé‡‡æ ·
        train_dataset = train_dataset.shuffle(seed=42).select(range(train_sample_size))

    if eval_sample_size:
        print(f"ğŸ”ª å¯¹è¯„ä¼°é›†è¿›è¡Œé‡‡æ ·ï¼Œä¿ç•™ {eval_sample_size} æ¡æ ·æœ¬...")
        eval_dataset = eval_dataset.shuffle(seed=42).select(range(eval_sample_size))

    print("âš™ï¸  æ­£åœ¨å¯¹é‡‡æ ·åçš„æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†...")
    # è·å–CPUæ ¸å¿ƒæ•°ç”¨äºå¹¶è¡Œå¤„ç†
    num_proc = os.cpu_count()
    print(f"ğŸ”§ ä½¿ç”¨ {num_proc} ä¸ªCPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œå¤„ç†...")
    
    # ä½¿ç”¨ .map() æ–¹æ³•é«˜æ•ˆåœ°åº”ç”¨é¢„å¤„ç†å‡½æ•°åˆ°é‡‡æ ·åçš„æ•°æ®
    # remove_columns ä¼šåˆ é™¤åŸå§‹çš„'translation'åˆ—ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å¤„ç†å®Œå¹¶ç”Ÿæˆäº†'input_ids', 'labels'ç­‰
    train_dataset = train_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=num_proc,
        remove_columns=raw_datasets["train"].column_names
    )
    
    eval_dataset = eval_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=num_proc,
        remove_columns=raw_datasets["validation"].column_names
    )

    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆã€‚è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, è¯„ä¼°é›†å¤§å°: {len(eval_dataset)}")

    # Seq2Seq ä»»åŠ¡ä¸è¿”å› num_labelsï¼Œæ‰€ä»¥è¿”å› None
    return train_dataset, eval_dataset, None
