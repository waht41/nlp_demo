# train.py
# ä¸»è®­ç»ƒè„šæœ¬ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—å¹¶å¯åŠ¨è®­ç»ƒ

from transformers import TrainingArguments, Trainer, AutoTokenizer

# ä»æˆ‘ä»¬åˆ›å»ºçš„æ¨¡å—ä¸­å¯¼å…¥å‡½æ•°
from src.model_handler import load_model_and_tokenizer
from src.data_handler import load_and_prepare_dataset
from src.metrics import compute_metrics


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹"""

    # --- 1. å®šä¹‰é…ç½® ---
    # æ¨¡å‹å’Œæ•°æ®é›†é…ç½®
    MODEL_CHECKPOINT = "distilbert-base-uncased"
    DATASET_NAME = "rotten_tomatoes"

    # ç›®å½•é…ç½®
    OUTPUT_DIR = "./results"
    LOGGING_DIR = "./logs"

    # å¿«é€Ÿæµ‹è¯•çš„é‡‡æ ·å¤§å° (è®¾ç½®ä¸ºNoneå¯ä½¿ç”¨å®Œæ•´æ•°æ®é›†)
    TRAIN_SAMPLE_SIZE = 1000
    EVAL_SAMPLE_SIZE = 200

    # --- 2. åŠ è½½æ•°æ®é›† ---
    # å…ˆåŠ è½½ä¸€æ¬¡åˆ†è¯å™¨ï¼Œç”¨äºæ•°æ®é¢„å¤„ç†
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

    train_dataset, eval_dataset, num_labels = load_and_prepare_dataset(
        dataset_name=DATASET_NAME,
        tokenizer=tokenizer,
        train_sample_size=TRAIN_SAMPLE_SIZE,
        eval_sample_size=EVAL_SAMPLE_SIZE
    )

    # --- 3. åŠ è½½æ¨¡å‹ ---
    # `_` æ¥æ”¶äº†å†æ¬¡åŠ è½½çš„åˆ†è¯å™¨ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†ï¼Œæ‰€ä»¥å¿½ç•¥å®ƒ
    model, _ = load_model_and_tokenizer(
        model_checkpoint=MODEL_CHECKPOINT,
        num_labels=num_labels
    )

    # --- 4. é…ç½®è®­ç»ƒå‚æ•° ---
    print("é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        # ç›®å½•å’ŒæŠ¥å‘Šé…ç½®
        output_dir=OUTPUT_DIR,
        logging_dir=LOGGING_DIR,
        report_to="tensorboard",  # å¯ç”¨TensorBoardæ—¥å¿—

        # è®­ç»ƒè¿‡ç¨‹é…ç½®
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=1e-5,
        weight_decay=0.01,
        max_grad_norm=1.0,  # æ¢¯åº¦è£å‰ª

        # è¯„ä¼°å’Œä¿å­˜ç­–ç•¥
        eval_strategy="epoch",  # æ¯ä¸ªepochç»“æŸåè¿›è¡Œè¯„ä¼°
        save_strategy="epoch",  # æ¯ä¸ªepochç»“æŸåä¿å­˜æ¨¡å‹
        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹

        # æ—¥å¿—è®°å½•
        logging_strategy="steps",
        logging_steps=10,  # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—åˆ°æ§åˆ¶å°å’ŒTensorBoard
    )

    # --- 5. åˆå§‹åŒ–å¹¶å¯åŠ¨è®­ç»ƒå™¨ ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    print("\n" + "=" * 40)
    print("          ğŸ”¥ å¼€å§‹æ¨¡å‹è®­ç»ƒ ğŸ”¥          ")
    print("=" * 40 + "\n")

    # å¯åŠ¨è®­ç»ƒ
    trainer.train()

    print("\n" + "=" * 40)
    print("          âœ… è®­ç»ƒå®Œæˆ âœ…          ")
    print("=" * 40 + "\n")

    # --- 6. åœ¨è¯„ä¼°é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° ---
    print("åœ¨æœ€ç»ˆè¯„ä¼°é›†ä¸Šè¿›è¡Œè¯„ä¼°...")
    final_metrics = trainer.evaluate(eval_dataset)
    print("æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(final_metrics)

    # --- 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = f"{OUTPUT_DIR}/final_model"
    trainer.save_model(final_model_path)
    print(f"\næœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
    print(f"è¦æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir {LOGGING_DIR}")


if __name__ == "__main__":
    main()
