# tasks/story_generation/data_handler.py

from datasets import load_dataset


def load_and_prepare_dataset(dataset_name, tokenizer, train_sample_size, eval_sample_size, max_length,
                             dataset_config_name=None, **kwargs):
    """
    ä¸º Causal LM ä»»åŠ¡åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†ã€‚

    Args:
        dataset_name (str): æ•°æ®é›†åç§°ã€‚
        tokenizer: Hugging Face Tokenizer å®ä¾‹ã€‚
        train_sample_size (int): è®­ç»ƒé›†é‡‡æ ·å¤§å°ã€‚
        eval_sample_size (int): è¯„ä¼°é›†é‡‡æ ·å¤§å°ã€‚
        max_length (int): åˆ†è¯æ—¶çš„æœ€å¤§é•¿åº¦ã€‚
        dataset_config_name (str, optional): æ•°æ®é›†çš„ç‰¹å®šé…ç½®ã€‚
        **kwargs: æ¥æ”¶ main.py ä¼ å…¥çš„å…¶ä»–å¤šä½™å‚æ•°ï¼Œä»¥ä¿æŒæ¥å£å…¼å®¹æ€§ã€‚

    Returns:
        tuple: (tokenized_train_dataset, tokenized_eval_dataset, num_labels)
               å¯¹äºCausalLMä»»åŠ¡, num_labels ä¸º Noneã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›† '{dataset_name}'...")
    # åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ŒåŒ…å«è®­ç»ƒé›†å’ŒéªŒè¯é›†
    full_dataset = load_dataset(dataset_name, name=dataset_config_name)
    train_dataset = full_dataset['train']
    eval_dataset = full_dataset['validation']

    # å¦‚æœæŒ‡å®šäº†é‡‡æ ·å¤§å°ï¼Œåˆ™å¯¹æ•°æ®é›†è¿›è¡Œé‡‡æ ·
    if train_sample_size:
        print(f"ğŸ” é‡‡æ · {train_sample_size} æ¡æ•°æ®ä½œä¸ºè®­ç»ƒé›†...")
        train_dataset = train_dataset.select(range(train_sample_size))
    
    if eval_sample_size:
        print(f"ğŸ” é‡‡æ · {eval_sample_size} æ¡æ•°æ®ä½œä¸ºè¯„ä¼°é›†...")
        eval_dataset = eval_dataset.select(range(eval_sample_size))

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, è¯„ä¼°é›†å¤§å°: {len(eval_dataset)}")

    def tokenize_function(examples):
        """åˆ†è¯å‡½æ•°ï¼Œå¤„ç† 'story' å­—æ®µ"""
        # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶æˆªæ–­åˆ° max_length
        # æ³¨æ„ï¼šè¿™é‡Œä¸è¿›è¡Œå¡«å……ï¼Œè®© DataCollator åœ¨æ‰¹å¤„ç†æ—¶ç»Ÿä¸€å¤„ç†
        output = tokenizer(
            examples["story"],
            truncation=True,
            max_length=max_length,
            padding=False,  # ä¸åœ¨è¿™é‡Œå¡«å……ï¼Œè®© DataCollator å¤„ç†
            return_special_tokens_mask=False,  # ä¸éœ€è¦ç‰¹æ®Štokenæ©ç 
        )
        # å¯¹äº CausalLM ä»»åŠ¡, labels å°±æ˜¯ input_ids çš„ä¸€ä¸ªå‰¯æœ¬
        # Trainer ä¼šè‡ªåŠ¨å¤„ç†å‘å³ç§»ä½ä»¥è¿›è¡Œä¸‹ä¸€ä¸ªtokené¢„æµ‹
        output["labels"] = output["input_ids"].copy()
        return output

    print("ğŸ§  æ­£åœ¨å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯...")
    tokenized_train_dataset = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=train_dataset.column_names
    )
    tokenized_eval_dataset = eval_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=eval_dataset.column_names
    )

    print("âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    # Causal LM ä»»åŠ¡æ²¡æœ‰ num_labels çš„æ¦‚å¿µï¼Œè¿”å› None
    return tokenized_train_dataset, tokenized_eval_dataset, None