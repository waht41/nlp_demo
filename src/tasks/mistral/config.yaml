# tasks/mistral-7b-lora-instruct/config.yaml

# ---------------------------------------------------
# 元数据: 用于描述和记录本次实验
# ---------------------------------------------------
metadata:
  run_name: "mistral-7b-lora-q4-ultrachat-subset"
  description: "使用 LoRA 和 4-bit 量化技术，在 UltraChat 数据集的一个小子集上微调 Mistral-7B 模型。"

# ---------------------------------------------------
# 模型与数据配置
# ---------------------------------------------------
model_data:
  # 关键！必须设置为 causalLM 才能触发 LLM 微调逻辑
  task_type: "causalLM"

  # Hugging Face Hub 上的基础模型名称
  # 注意: 加载此模型可能需要你先登录 Hugging Face CLI: huggingface-cli login
  model_checkpoint: "mistralai/Mistral-7B-v0.1"

  # Hugging Face Hub 上的数据集名称
  dataset_name: "HuggingFaceH4/ultrachat_200k"

  # 为了快速实验，只使用一小部分数据。在正式训练时可以注释掉或增大数值。
  train_sample_size: 2000
  eval_sample_size: 200

  # 输入给模型的最大序列长度
  max_length: 1024

# ---------------------------------------------------
# 训练过程配置
# ---------------------------------------------------
training:
  # --- 核心开关：启用 PEFT (LoRA) ---
  use_peft: true

  # --- 量化配置：以更低的精度加载模型以节省显存 ---
  quantization:
    load_in_4bit: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: "nf4"
    # 对于 NVIDIA Ampere (A100, RTX 30xx/40xx) 或更新的 GPU，使用 bfloat16
    # 对于旧款 GPU (V100, T4)，请将其改为 "float16"
    bnb_4bit_compute_dtype: "bfloat16"

  # --- PEFT LoRA 参数配置 ---
  peft_lora:
    r: 16               # LoRA 的秩 (rank)，是关键参数。常见值: 8, 16, 32, 64
    lora_alpha: 32      # LoRA 的缩放因子，通常设置为 r 的两倍

    # ！！！极其重要: 指定要应用 LoRA 的模块名。
    # 这个列表是模型特定的！你需要查看模型架构来确定。
    # Mistral-7B 和 LLaMA 模型的 attention 模块通常是下面这四个。
    # 对于 BLOOM 模型，可能是 ["query_key_value"]。
    # 如何查找: 加载模型后打印 model.named_modules() 查看所有模块名。
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"

    lora_dropout: 0.05  # LoRA 层的 dropout 比例
    bias: "none"        # 是否训练偏置项，"none"、"all" 或 "lora_only"
    task_type: "CAUSAL_LM" # 任务类型，对于 Decoder-Only 模型是 "CAUSAL_LM"

  # --- 标准训练超参数 ---
  # 使用专门为量化模型设计的 paged optimizer
  optimizer: "paged_adamw_8bit"

  num_train_epochs: 1   # 指令微调通常不需要太多 epoch，1-3 个即可
  per_device_train_batch_size: 1 # 对于 7B 模型，在 24GB VRAM 上通常只能设为 1
  gradient_accumulation_steps: 8 # 梯度累积，模拟更大的批次大小。有效批次大小 = 1 * 8 = 8

  learning_rate: 2.0e-4 # LoRA 微调的学习率通常比全量微调要高
  lr_scheduler_type: "cosine" # 学习率调度器类型
  warmup_ratio: 0.1     # 学习率预热比例

  weight_decay: 0.01
  max_grad_norm: 1.0

  # 训练时使用 bfloat16，与量化配置中的 compute_dtype 保持一致
  # 如果你的 GPU 不支持 bfloat16，请将此项改为 fp16: true
  bf16: true
  fp16: false

  # --- 日志、评估和保存策略 ---
  logging_strategy: "steps"
  logging_steps: 10
  eval_strategy: "epoch"   # 每个 epoch 结束时进行一次评估
  save_strategy: "epoch"   # 每个 epoch 结束时保存一次检查点
  load_best_model_at_end: true

  # --- 其他回调开关 ---
  log_ppl: true           # 是否在日志中记录困惑度(Perplexity)
  ignore_compute_metric: true # 对于 LLM 生成任务，标准的分类指标不适用，可以忽略