# train.py
# ä¸»è®­ç»ƒè„šæœ¬ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—å¹¶å¯åŠ¨è®­ç»ƒ
from datetime import datetime

from transformers import TrainingArguments, Trainer, AutoTokenizer

# ä»æˆ‘ä»¬åˆ›å»ºçš„æ¨¡å—ä¸­å¯¼å…¥å‡½æ•°
from src.model_handler import load_model_and_tokenizer
from src.data_handler import load_and_prepare_dataset
from src.metrics import compute_metrics


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹"""

    # --- 1. å®šä¹‰é…ç½® ---
    # æ¨¡å‹å’Œæ•°æ®é›†é…ç½®
    MODEL_CHECKPOINT = "distilbert-base-uncased"
    DATASET_NAME = "rotten_tomatoes"

    # å¿«é€Ÿæµ‹è¯•çš„é‡‡æ ·å¤§å° (è®¾ç½®ä¸ºNoneå¯ä½¿ç”¨å®Œæ•´æ•°æ®é›†)
    TRAIN_SAMPLE_SIZE = None
    EVAL_SAMPLE_SIZE = None

    # ä¸ºæ—¥å¿—åˆ›å»ºåç§°
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    learning_rate = 1e-6
    run_name = f"lr-{learning_rate}_{timestamp}"

    # ç›®å½•é…ç½® - ä¸ºæ¯ä¸ªè®­ç»ƒè¿è¡Œåˆ›å»ºç‹¬ç«‹æ–‡ä»¶å¤¹
    OUTPUT_DIR = f"./results/{run_name}"
    LOGGING_DIR = f"./logs/{run_name}"


    # --- 2. åŠ è½½æ•°æ®é›† ---
    # å…ˆåŠ è½½ä¸€æ¬¡åˆ†è¯å™¨ï¼Œç”¨äºæ•°æ®é¢„å¤„ç†
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

    train_dataset, eval_dataset, num_labels = load_and_prepare_dataset(
        dataset_name=DATASET_NAME,
        tokenizer=tokenizer,
        train_sample_size=TRAIN_SAMPLE_SIZE,
        eval_sample_size=EVAL_SAMPLE_SIZE
    )
    print(f"ä»æ•°æ®é›†ä¸­æ¨æ–­å‡ºçš„ num_labels: {num_labels}")

    print("\n" + "=" * 20 + " æœ€ç»ˆæ•°æ®é›†éªŒè¯ " + "=" * 20)
    print("--- è®­ç»ƒé›†ä¿¡æ¯ ---")
    print(train_dataset)
    print("è®­ç»ƒé›†ç‰¹å¾ (Features):")
    print(train_dataset.features)  # é‡ç‚¹æ£€æŸ¥è¿™é‡Œï¼Œç¡®è®¤æœ‰ 'labels' ä¸”ç±»å‹æ­£ç¡®

    print("\n--- æŠ½æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬ ---")
    sample = train_dataset[0]
    print(sample)
    # ç¡®è®¤ 'labels' é”®å­˜åœ¨ï¼Œå¹¶ä¸”å€¼æ˜¯ä¸€ä¸ª tensor æ•´æ•°
    print(f"æ ·æœ¬ä¸­ 'labels' çš„å€¼: {sample['labels']}")
    print(f"æ ·æœ¬ä¸­ 'labels' çš„æ•°æ®ç±»å‹: {sample['labels'].dtype}")
    print("=" * 40 + "\n")
    # ===============================================================

    # --- 3. åŠ è½½æ¨¡å‹ ---
    # `_` æ¥æ”¶äº†å†æ¬¡åŠ è½½çš„åˆ†è¯å™¨ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†ï¼Œæ‰€ä»¥å¿½ç•¥å®ƒ
    model, _ = load_model_and_tokenizer(
        model_checkpoint=MODEL_CHECKPOINT,
        num_labels=num_labels
    )

    # --- 4. é…ç½®è®­ç»ƒå‚æ•° ---
    print("é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        # ç›®å½•å’ŒæŠ¥å‘Šé…ç½®
        output_dir=OUTPUT_DIR,
        logging_dir=LOGGING_DIR,
        report_to="tensorboard",  # å¯ç”¨TensorBoardæ—¥å¿—
        run_name=run_name,

        # è®­ç»ƒè¿‡ç¨‹é…ç½®
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=learning_rate,
        weight_decay=0.01,
        max_grad_norm=1.0,  # æ¢¯åº¦è£å‰ª
        warmup_ratio=0.1,

        # è¯„ä¼°å’Œä¿å­˜ç­–ç•¥
        eval_strategy="epoch",  # æ¯ä¸ªepochç»“æŸåè¿›è¡Œè¯„ä¼°
        save_strategy="epoch",  # æ¯ä¸ªepochç»“æŸåä¿å­˜æ¨¡å‹
        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹

        # æ—¥å¿—è®°å½•
        logging_strategy="steps",
        logging_steps=10,  # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—åˆ°æ§åˆ¶å°å’ŒTensorBoard
    )

    # --- 5. åˆå§‹åŒ–å¹¶å¯åŠ¨è®­ç»ƒå™¨ ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    print("\n" + "=" * 40)
    print("          ğŸ”¥ å¼€å§‹æ¨¡å‹è®­ç»ƒ ğŸ”¥          ")
    print("=" * 40 + "\n")

    # å¯åŠ¨è®­ç»ƒ
    trainer.train()

    print("\n" + "=" * 40)
    print("          âœ… è®­ç»ƒå®Œæˆ âœ…          ")
    print("=" * 40 + "\n")

    # --- 6. åœ¨è¯„ä¼°é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° ---
    print("åœ¨æœ€ç»ˆè¯„ä¼°é›†ä¸Šè¿›è¡Œè¯„ä¼°...")
    final_metrics = trainer.evaluate(eval_dataset)
    print("æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(final_metrics)

    # --- 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = f"{OUTPUT_DIR}/final_model"
    trainer.save_model(final_model_path)
    print(f"\næœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
    print(f"è¦æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir {LOGGING_DIR}")


if __name__ == "__main__":
    main()
